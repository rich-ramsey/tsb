@Article{munafo2017,
  title = {A Manifesto for Reproducible Science},
  author = {Marcus R. Munafò and Brian A. Nosek and Dorothy V. M. Bishop and Katherine S. Button and Christopher D. Chambers and Nathalie {Percie du Sert} and Uri Simonsohn and Eric-Jan Wagenmakers and Jennifer J. Ware and John P. A. Ioannidis},
  year = {2017},
  journaltitle = {Nature Human Behaviour},
  volume = {1},
  pages = {0021},
  doi = {10.1038/s41562-016-0021},
  url = {http://dx.doi.org/10.1038/s41562-016-0021},
}
@article{meehl1967,
  title = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}: {{A Methodological Paradox}}},
  shorttitle = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}},
  author = {Meehl, Paul E.},
  date = {1967-06},
  journaltitle = {Philosophy of Science},
  volume = {34},
  number = {2},
  pages = {103--115},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/288135},
  url = {https://www.cambridge.org/core/journals/philosophy-of-science/article/abs/theorytesting-in-psychology-and-physics-a-methodological-paradox/F6CDBDBE5D960C356178BDA5D87EB4BC},
  urldate = {2025-05-26},
  abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching ½ of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by “success” is very weak, and becomes weaker with increased precision. “Statistical significance” plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental “cuteness” and a free reliance upon ad hoc explanations to avoid refutation.},
  langid = {english}
}
@incollection{hintzman1991,
  title = {Why Are Formal Models Useful in Psychology?},
  booktitle = {Relating Theory and Data: {{Essays}} on Human Memory in Honor of {{Bennet B}}. {{Murdock}}.},
  author = {Hintzman, Douglas L.},
  date = {1991},
  pages = {39--56},
  publisher = {Lawrence Erlbaum Associates, Inc},
  location = {Hillsdale, NJ, US},
  abstract = {explores the value of formal (mathematical and computer) models in psychology / research on factors that have been shown to bias and limit unaided human reasoning is briefly reviewed, and it is noted that psychologists are susceptible to these errors, in an effort to identify the ways in which models can and cannot aid scientific thought / some limitations of the modeling approach are also discussed the following discussion has four parts / first, I list several sources of error in unaided human reasoning; second, I discuss the nature of formal models; third, I attempt to relate models to reasoning errors, to uncover where the advantages of modeling might lie / finally, I consider the evaluation of formal models, and argue that there are limitations as well as advantages in their use (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {0-8058-0732-2 (Hardcover); 0-8058-0733-0 (Paperback)},
  keywords = {*Computer Applications,*Mathematical Modeling,Reasoning}
}
@article{ward2024,
  title = {Integrating {{Social Cognition Into Domain-General Control}}: {{Interactive Activation}} and {{Competition}} for the {{Control}} of {{Action}} ({{ICON}})},
  shorttitle = {Integrating {{Social Cognition Into Domain-General Control}}},
  author = {Ward, Robert and Ramsey, Richard},
  date = {2024},
  journaltitle = {Cognitive Science},
  volume = {48},
  number = {2},
  pages = {e13415},
  issn = {1551-6709},
  doi = {10.1111/cogs.13415},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13415},
  urldate = {2024-02-26},
  abstract = {Social cognition differs from general cognition in its focus on understanding, perceiving, and interpreting social information. However, we argue that the significance of domain-general processes for controlling cognition has been historically undervalued in social cognition and social neuroscience research. We suggest much of social cognition can be characterized as specialized feature representations supported by domain-general cognitive control systems. To test this proposal, we develop a comprehensive working model, based on an interactive activation and competition architecture and applied to the control of action. As such, we label the model “ICON” (interactive activation and competition model for the control of action). We used the ICON model to simulate human performance across various laboratory tasks. Our simulations emphasize that many laboratory-based social tasks do not require socially specific control systems, such as those that are argued to rely on neural networks associated with theory-of-mind. Moreover, our model clarifies that perceived disruptions in social cognition, even in what appears to be disruption to the control of social cognition, can stem from deficits in social representation instead. We advocate for a “default stance” in social cognition, where control is usually general, but representation is specific. This study underscores the importance of integrating social cognition within the broader realm of domain-general control processing, offering a unified perspective on task processing.},
  langid = {english},
  keywords = {Approach–avoid,Autism spectrum,Computational modeling,Control of action,Imitation,Social cognition,Stimulus–response compatibility},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Ward_Ramsey_2024_Integrating Social Cognition Into Domain-General ControlIntegrating Social Cognition Into Domain-General Control.pdf;/Users/rramsey/Zotero/storage/UEGSN5GQ/cogs.html}
}
@article{orben2019,
  title = {The Association between Adolescent Well-Being and Digital Technology Use},
  author = {Orben, Amy and Przybylski, Andrew K.},
  date = {2019-02},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {3},
  number = {2},
  pages = {173--182},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0506-1},
  url = {https://www.nature.com/articles/s41562-018-0506-1},
  urldate = {2025-05-26},
  abstract = {The widespread use of digital technologies by young people has spurred speculation that their regular use negatively impacts psychological well-being. Current empirical evidence supporting this idea is largely based on secondary analyses of large-scale social datasets. Though these datasets provide a valuable resource for highly powered investigations, their many variables and observations are often explored with an analytical flexibility that marks small effects as statistically significant, thereby leading to potential false positives and conflicting results. Here we address these methodological challenges by applying specification curve analysis (SCA) across three large-scale social datasets (total n\,=\,355,358) to rigorously examine correlational evidence for the effects of digital technology on adolescents. The association we find between digital technology use and adolescent well-being is negative but small, explaining at most 0.4\% of the variation in well-being. Taking the broader context of the data into account suggests that these effects are too small to warrant policy change.},
  langid = {english},
  keywords = {Human behaviour},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Orben_Przybylski_2019_natureHumanBehaviour_the_association_between_adolescent_well-being_and_digital_technology_use.pdf}
}
@article{gray2017,
  title = {How to {{Map Theory}}: {{Reliable Methods Are Fruitless Without Rigorous Theory}}},
  author = {Gray, K.},
  date = {2017-09},
  journaltitle = {Perspect Psychol Sci},
  volume = {12},
  number = {5},
  eprint = {28873328},
  eprinttype = {pubmed},
  pages = {731--741},
  issn = {1745-6924 (Electronic) 1745-6916 (Linking)},
  doi = {10.1177/1745691617691949},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/28873328},
  abstract = {Good science requires both reliable methods and rigorous theory. Theory allows us to build a unified structure of knowledge, to connect the dots of individual studies and reveal the bigger picture. Some have criticized the proliferation of pet "Theories," but generic "theory" is essential to healthy science, because questions of theory are ultimately those of validity. Although reliable methods and rigorous theory are synergistic, Action Identification suggests psychological tension between them: The more we focus on methodological details, the less we notice the broader connections. Therefore, psychology needs to supplement training in methods (how to design studies and analyze data) with training in theory (how to connect studies and synthesize ideas). This article provides a technique for visually outlining theory: theory mapping. Theory mapping contains five elements, which are illustrated with moral judgment and with cars. Also included are 15 additional theory maps provided by experts in emotion, culture, priming, power, stress, ideology, morality, marketing, decision-making, and more (see all at theorymaps.org ). Theory mapping provides both precision and synthesis, which helps to resolve arguments, prevent redundancies, assess the theoretical contribution of papers, and evaluate the likelihood of surprising effects.},
  keywords = {action identification,morality,scientific methodology,social cognition,theory}
}
@article{scheel2021,
  title = {Why Hypothesis Testers Should Spend Less Time Testing Hypotheses},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Daniël},
  date = {2021},
  journaltitle = {Perspectives on Psychological Science},
  volume = {16},
  pages = {744--755},
  publisher = {Sage Publications},
  location = {US},
  issn = {1745-6924},
  doi = {10.1177/1745691620966795},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound “derivation chain” between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology’s reform movement and help us to develop strong, testable theories, as Paul Meehl urged. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Auditory Stimulation,Concepts,Experimental Replication,Hypothesis Testing,Measurement,Social Sciences,Test Validity,Theories},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Scheel_2021_Why hypothesis testers should spend less time testing hypothesesWhy hypothesis testers should spend less time testing hypotheses.pdf;/Users/rramsey/Zotero/storage/SQRRKF38/2021-66119-007.html}
}
@article{flake2020,
  title = {Measurement Schmeasurement: {{Questionable}} Measurement Practices and How to Avoid Them},
  shorttitle = {Measurement Schmeasurement},
  author = {Flake, Jessica Kay and Fried, Eiko I.},
  date = {2020},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  pages = {456--465},
  publisher = {Sage Publications},
  location = {US},
  issn = {2515-2467},
  doi = {10.1177/2515245920952393},
  abstract = {In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study’s inferences, and are necessary for meaningful replication studies. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Behavioral Sciences,Construct Validity,Experimental Replication,Measurement,Psychometrics,Test Validity},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Flake_Fried_2020_Measurement schmeasurementMeasurement schmeasurement.pdf;/Users/rramsey/Zotero/storage/MCGBCTF5/2020-98063-002.html}
}
@article{baron-cohen2001,
  title = {The “{{Reading}} the {{Mind}} in the {{Eyes}}” {{Test Revised Version}}: {{A Study}} with {{Normal Adults}}, and {{Adults}} with {{Asperger Syndrome}} or {{High-functioning Autism}}},
  shorttitle = {The “{{Reading}} the {{Mind}} in the {{Eyes}}” {{Test Revised Version}}},
  author = {Baron-Cohen, Simon and Wheelwright, Sally and Hill, Jacqueline and Raste, Yogini and Plumb, Ian},
  date = {2001},
  journaltitle = {Journal of Child Psychology and Psychiatry},
  volume = {42},
  number = {2},
  pages = {241--251},
  issn = {1469-7610},
  doi = {10.1111/1469-7610.00715},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1469-7610.00715},
  urldate = {2025-05-26},
  abstract = {In 1997 in this Journal we published the “Reading the Mind in the Eyes” Test, as a measure of adult “mentalising”. Whilst that test succeeded in discriminating a group of adults with Asperger syndrome (AS) or high-functioning autism (HFA) from controls, it suffered from several psychometric problems. In this paper these limitations are rectified by revising the test. The Revised Eyes Test was administered to a group of adults with AS or HFA (N= 15) and again discriminated these from a large number of normal controls (N= 239) drawn from different samples. In both the clinical and control groups the Eyes Test was inversely correlated with the Autism Spectrum Quotient (the AQ), a measure of autistic traits in adults of normal intelligence. The Revised Eyes Test has improved power to detect subtle individual differences in social sensitivity.},
  langid = {english},
  keywords = {Asperger's Disorder,autistic disorder,social cognition,Theory of mind},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Baron-Cohen_Wheelwright_Hill_2001_journalOfChildPsychologyAndPsychiatry_the_“reading_the_mind_in_the_eyes”_test_revised_version_a_study_with_normal_adults,_and_adults_with.pdf}
}
@article{higgins2022,
  title = {The “{{Reading}} the {{Mind}} in the {{Eyes}}” {{Test Shows Poor Psychometric Properties}} in a {{Large}}, {{Demographically Representative U}}.{{S}}. {{Sample}}},
  author = {Higgins, Wendy C. and Ross, Robert M. and Langdon, Robyn and Polito, Vince},
  date = {2022-09-19},
  journaltitle = {Assessment},
  shortjournal = {Assessment},
  pages = {10731911221124342},
  publisher = {SAGE Publications Inc},
  issn = {1073-1911},
  doi = {10.1177/10731911221124342},
  url = {https://doi.org/10.1177/10731911221124342},
  urldate = {2023-02-20},
  abstract = {The Reading the Mind in the Eyes test (RMET) is a widely used measure of theory of mind (ToM). Despite its popularity, there are questions regarding the RMET?s psychometric properties. In the current study, we examined the RMET in a representative U.S. sample of 1,181 adults. Key analyses included conducting an exploratory factor analysis on the full sample and examining whether there is a different factor structure in individuals with high versus low scores on the 28-item autism spectrum quotient (AQ-28). We identified overlapping, but distinct, three-factor models for the full sample and the two subgroups. In all cases, each of the three models showed inadequate model fit. We also found other limitations of the RMET, including that nearly a quarter of the RMET items did not meet the criteria for inclusion in the RMET that were established in the original validation study. Due to the RMET?s weak psychometric properties and the uncertain validity of individual items, as indicated by our study and previous studies, we conclude that significant caution is warranted when using the RMET as a measure of ToM.},
  langid = {english},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Higgins_2022_AssessmentThe “Reading the Mind in the Eyes” Test Shows Poor Psychometric Properties in a.pdf}
}
@article{guestHowComputationalModeling2021,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  date = {2021},
  journaltitle = {Perspectives on Psychological Science},
  pages = {1745691620970585},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  url = {https://doi.org/10.1177/1745691620970585},
  urldate = {2021-04-18},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined?what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.}
}
@article{yarkoni2022,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  date = {2022},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  publisher = {Cambridge University Press},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20001685},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/generalizability-crisis/AD386115BA539A759ACB3093760F4824},
  urldate = {2023-03-24},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned – that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology – the linear mixed model – I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the “random effect” formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  keywords = {Generalization,inference,philosophy of science,psychology,random effects,statistics},
  file = {/Users/rramsey/Dropbox/docs/journals/cog_neuro/Yarkoni_2022_The generalizability crisisThe generalizability crisis.pdf}
}
@book{farrell2018,
  title = {Computational {{Modeling}} of {{Cognition}} and {{Behavior}}:},
  shorttitle = {Computational {{Modeling}} of {{Cognition}} and {{Behavior}}},
  author = {Farrell, Simon and Lewandowsky, Stephan},
  date = {2018-02-22},
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781316272503},
  url = {https://www.cambridge.org/core/product/identifier/9781316272503/type/book},
  urldate = {2025-01-08},
  isbn = {978-1-107-10999-5 978-1-316-27250-3 978-1-107-52561-0},
  langid = {english},
  file = {/Users/rramsey/Zotero/storage/S6R7XWSA/Farrell and Lewandowsky - 2018 - Computational Modeling of Cognition and Behavior.pdf}
}

