@Article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  date = {2015-08-28},
  journaltitle = {Science},
  volume = {349},
  number = {6251},
  doi = {10.1126/science.aac4716},
  url = {http://www.sciencemag.org/content/349/6251/aac4716.abstract},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
}
@Article{munafo2017,
  title = {A Manifesto for Reproducible Science},
  author = {Marcus R. Munaf√≤ and Brian A. Nosek and Dorothy V. M. Bishop and Katherine S. Button and Christopher D. Chambers and Nathalie {Percie du Sert} and Uri Simonsohn and Eric-Jan Wagenmakers and Jennifer J. Ware and John P. A. Ioannidis},
  year = {2017},
  journaltitle = {Nature Human Behaviour},
  volume = {1},
  pages = {0021},
  doi = {10.1038/s41562-016-0021},
  url = {http://dx.doi.org/10.1038/s41562-016-0021},
}
@Article{simmons2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  author = {J. P. Simmons and L. D. Nelson and U. Simonsohn},
  date = {2011-11},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  url = {://WOS:000300826400001},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$<$}= .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {disclosure,methodology,motivated reasoning,publication},
}